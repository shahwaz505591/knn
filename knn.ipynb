{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a302aa7-4d89-4087-bf01-8d1d94296caa",
   "metadata": {},
   "source": [
    "Q1: KNN Algorithm:\n",
    "K-Nearest Neighbors (KNN) is a machine learning algorithm used for both classification and regression tasks. It makes predictions based on the majority class (for classification) or the average of the nearest neighbors (for regression) in the feature space. The \"k\" in KNN represents the number of nearest neighbors to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e87064-cfc0-4949-9d6f-c32114ba72c3",
   "metadata": {},
   "source": [
    "Q2: Choosing the Value of K in KNN:\n",
    "The choice of the value of K in KNN is a crucial decision. A small K (e.g., 1 or 3) can make the model sensitive to noise, while a large K may make the model too biased. You can choose K using techniques like cross-validation. Common values for K include odd numbers to prevent ties, but the optimal K depends on the specific dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e2b07-3cf5-4091-aabc-19077ba3a2d7",
   "metadata": {},
   "source": [
    "Q3: Difference Between KNN Classifier and KNN Regressor:\n",
    "\n",
    "KNN Classifier: Used for classification tasks, where it predicts the class or category of a data point based on the majority class among its K-nearest neighbors.\n",
    "KNN Regressor: Used for regression tasks, where it predicts a continuous value based on the average (or weighted average) of the target values of its K-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611a635-ea68-45c7-a884-7d5ec4e194da",
   "metadata": {},
   "source": [
    "Q4: Measuring KNN Performance:\n",
    "The performance of KNN can be measured using various evaluation metrics, depending on whether it's used for classification or regression tasks. Common metrics include accuracy, precision, recall, F1-score (for classification), and mean squared error (MSE) or R-squared (for regression). Cross-validation is often employed to estimate the model's performance more reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265f2d6-ad0b-40b3-bd0d-4e7c0f36a3b2",
   "metadata": {},
   "source": [
    "Q5: Curse of Dimensionality in KNN:\n",
    "The \"Curse of Dimensionality\" refers to the challenge of high-dimensionality in KNN. As the number of dimensions (features) increases, the distance between data points becomes less discriminative. This can result in all data points being roughly equidistant from a query point, making KNN less effective. Dimensionality reduction techniques, feature selection, or feature engineering can be used to address this issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188043a-2158-4c8d-9164-1e603dc35ba1",
   "metadata": {},
   "source": [
    "Q6: Handling Missing Values in KNN:\n",
    "Handling missing values in KNN depends on the context. You can either impute missing values (e.g., by replacing them with the mean, median, or mode) or consider them in the distance calculation as a special case (e.g., by assigning a high distance). The choice depends on the specific problem and the nature of the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc0665-c92c-4986-8ed0-294ba56ea02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
